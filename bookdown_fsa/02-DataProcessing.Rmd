# Data Cleaning and Preprocessing

## Cleaning

First step of our code is too import the data provided by FSA in Excel Format and to load it in a DataFrame.
This will allow us to perform the data cleaning, which is composed of the following steps:

- Remove of undesiered columns
- Ordering of data per dates
- Lowercasing of content
- Removal of undesired characters from including Emoticons, Hashtags, URLs, HTML tags and symbols and punctuation
- Removal of duplicates
- Removal of spaces around conten (Trimming)

For further future analysis, some information like Username and Hashtags are extracted in inserted in a new column.

Lots of content is composed of abbraviations. In order to be the more relevant possible in the future analysis, we convert those to there expander meaning for e.g. "asap" becomes "as soon as possible"


## Normalization

The normalization  refers to the transformation of words into a more uniform form. 

We perform stop words removals where words like "the" "an" and other articles are removed from the content. This allows compuation to be more efficiant, reducing content size.

We apply the stemming process, an algorithm that converts inflected forms of words into their base forms (stems). This allows us to discard variations of words (like singular, plural).

The words are treated like normalized elements that we call tokens.

We need to convert this set of tokens to a corpus in order to perform matricial analysis.


## Data Term Matrix

After all our data is cleaned and preprocessed, our corpus is converted into a Data Term Matrix. A DTM is a matrix in which rows are documents, columns are terms, and cells indicate how often each term occurred in each document. In our case each line represents a tweet, forum entry or news.

This is the base element that we will use for Stream 1 and Stream 2 analyis.








<!-- If you need PDF output, uncomment bookdown::pdf_book above in YAML. You will need a LaTeX installation, e.g., https://yihui.name/tinytex/ -->
